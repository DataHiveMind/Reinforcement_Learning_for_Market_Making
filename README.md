# Reinforcement Learning for Market Making

## ğŸ“Œ Problem Statement
In modern electronic markets, **market makers** are essential liquidity providers. They continuously quote bid/ask prices to facilitate trading, earning the spread while managing **inventory risk**, **adverse selection**, and **execution uncertainty**.  

The challenge:  
- **Microstructure complexity** â€” queue priority, partial fills, cancellations, and latency.  
- **Dynamic regimes** â€” volatility spikes, spread changes, liquidity droughts.  
- **Risk constraints** â€” inventory limits, capital usage, and tailâ€‘risk control.  

Traditional closedâ€‘form models (e.g., Avellanedaâ€“Stoikov) assume stationary dynamics and parametric orderâ€‘flow models. In reality, orderâ€‘flow is **nonâ€‘stationary** and **pathâ€‘dependent**.  

This project develops a **TensorFlowâ€‘based Reinforcement Learning (RL) framework** to learn adaptive quoting policies that maximize **riskâ€‘adjusted PnL** under realistic market microstructure constraints.

---

## ğŸ›  Quant Research Workflow

1. **Market Data Acquisition & Calibration**
   - Pull historical L2 order book and macro factors via `pandas-datareader` and `gs-quant`.
   - Fit volatility regimes, order arrival intensities, and spread transition matrices using `statsmodels`.

2. **Market Microstructure Simulation**
   - Gymâ€‘style Limit Order Book (LOB) environment with:
     - Queueâ€‘positionâ€‘aware fills
     - Latency and slippage modeling
     - Maker/taker fees and rebates
   - Configurable for asset, tick size, and regime.

3. **Feature Engineering**
   - LOB depth tensors, orderâ€‘flow imbalance, realized volatility, factor exposures.
   - Normalized and batched as `tf.Tensor` for RL agents.

4. **Baseline Strategies**
   - **Fixedâ€‘Spread with inventory skew**
   - **Avellanedaâ€“Stoikov** optimal control

5. **RL Agent Development**
   - **Soft Actorâ€‘Critic (SAC)** in TensorFlow/Keras.
   - `tensorflow-probability` for stochastic policy heads.
   - Reward = PnL âˆ’ Î»Â·InventoryPenalty âˆ’ Î·Â·CrossPenalty âˆ’ ÏˆÂ·TurnoverPenalty.

6. **Evaluation & Stress Testing**
   - Metrics: Sharpe, Sortino, VaR, ES, drawdowns, fill rate, realized spread.
   - Stress scenarios: volatility spikes, liquidity droughts, latency shocks.

7. **Reporting**
   - `quantstats` tear sheets for performance.
   - `Riskfolio-lib` for portfolioâ€‘style inventory risk analysis.

---

## ğŸ—º System Architecture (Quant Flow)

```mermaid
flowchart TD
    subgraph Data Layer
        A[Market Data Sources] --> B[Data Loaders]
        B --> C[Calibration Models]
    end

    subgraph Simulation Layer
        C --> D[LOB Environment]
        D --> E[Feature Engineering]
    end

    subgraph Agent Layer
        E --> F[RL Agent (SAC)]
        F -->|Quotes| D
    end

    subgraph Evaluation Layer
        D --> G[Evaluation & Stress Tests]
        G --> H[Reporting & Analytics]
    end

## ğŸ’» Tech Stack
Component	Technology	Why	How
Core Language	Python 3.12	Industry standard for quant research	All modules
Deep Learning	TensorFlow 2.x, tensorflow-probability	Productionâ€‘ready DL + probabilistic modeling	RL agent networks, stochastic policies
Market Data	pandas-datareader, gs-quant	Historical prices, macro factors, risk exposures	Data ingestion, factor analysis
Statistical Modeling	statsmodels	Econometrics, time series	Volatility regime calibration
RL Environment	Gymnasium API	Standard RL interface	LOB simulator
Performance Analytics	quantstats	Risk/return analytics	Tear sheets
Portfolio Risk	Riskfolio-lib	Portfolio optimization, CVaR	Inventory risk analysis
Experiment Tracking	MLflow	Reproducibility	Configs, metrics, artifacts
Visualization	Matplotlib, Seaborn	Data & performance plots	Calibration, evaluation


## ğŸ“Š Results (TBA)
Metric	RL Agent (SAC)	Avellanedaâ€“Stoikov	Fixed Spread
Annualized Sharpe	TBA	TBA	TBA
Sortino Ratio	TBA	TBA	TBA
Max Drawdown	TBA	TBA	TBA
99% Expected Shortfall	TBA	TBA	TBA
Fill Rate (%)	TBA	TBA	TBA
Realized Spread (bps)	TBA	TBA	TBA

## ğŸ”Œ ASCII Module Interaction Map
[ data/loaders.py ]
    â””â”€ Output: Pandas DataFrame (raw L2/L3 order book, trades, macro factors)
        â†“
[ data/preprocess.py ]
    â””â”€ Output: Cleaned Pandas DataFrame (aligned timestamps, filtered assets)
        â†“
[ calibration/vol_models.py ]
    â””â”€ Input: Pandas DataFrame (midprice returns)
    â””â”€ Output: Dict of calibrated parameters (vol regimes, GARCH params)
        â†“
[ calibration/microstructure.py ]
    â””â”€ Input: Pandas DataFrame (order flow, spreads)
    â””â”€ Output: Dict (arrival intensities, fill probabilities, spread transitions)
        â†“
[ envs/lob_env.py ]
    â””â”€ Input: Calibration dicts + config YAML
    â””â”€ Output: Python dict (LOB state: depth arrays, inventory, PnL)
        â†“
[ envs/fillers.py / envs/dynamics.py / envs/costs.py ]
    â””â”€ Input: Python dict (LOB state)
    â””â”€ Output: Updated Python dict (post-fill state, costs applied)
        â†“
[ features/lob_features.py ]
    â””â”€ Input: Python dict (LOB state)
    â””â”€ Output: NumPy array (depth snapshot tensor)
[ features/imbalance.py ]
    â””â”€ Output: NumPy array (order-flow imbalance features)
[ features/volatility.py ]
    â””â”€ Output: NumPy array (realized vol features)
[ features/factor_exposures.py ]
    â””â”€ Output: NumPy array (factor loadings from gs_quant)
        â†“
[ Feature Aggregation Layer ]
    â””â”€ Input: Multiple NumPy arrays
    â””â”€ Output: tf.Tensor (batched state representation for RL agent)
        â†“
[ agents/tf_sac/policy.py ]
    â””â”€ Input: tf.Tensor (state)
    â””â”€ Output: tf.Tensor (action logits or continuous offsets)
[ agents/tf_sac/learner.py ]
    â””â”€ Input: tf.Tensor (state, action, reward, next_state)
    â””â”€ Output: Updated TF model weights
[ agents/tf_sac/replay.py ]
    â””â”€ Input: Python tuple (state, action, reward, next_state, done)
    â””â”€ Output: Sampled batches as tf.Tensor for training
        â†“
[ agents/baselines/*.py ]
    â””â”€ Input: NumPy array or Pandas DataFrame (state features)
    â””â”€ Output: Python dict (quote offsets, sizes)
        â†“
[ experiments/scripts/train.py ]
    â””â”€ Orchestrates: env â†” agent loop, logs to MLflow
[ experiments/scripts/eval.py / stress_test.py ]
    â””â”€ Output: Pandas DataFrame (PnL paths, metrics)
        â†“
[ evaluation/performance.py ]
    â””â”€ Input: Pandas DataFrame (PnL, trades)
    â””â”€ Output: QuantStats HTML report, Riskfolio-lib portfolio metrics
[ evaluation/stress_tests.py ]
    â””â”€ Output: Pandas DataFrame (stress scenario results)
[ evaluation/reporting.py ]
    â””â”€ Output: PDF/HTML dashboards
        â†“
[ notebooks/*.ipynb ]
    â””â”€ Input: Pandas DataFrame, QuantStats reports
    â””â”€ Output: Visualizations, exploratory analysis
[ tracking/mlflow_utils.py ]
    â””â”€ Input: Metrics, params, artifacts
    â””â”€ Output: MLflow experiment logs


## ğŸš€ How to Run
# Clone repo
git clone https://github.com/yourusername/rl-mm.git
cd rl-mm

# Create venv
python3.12 -m venv .venv
source .venv/bin/activate

# Install deps
pip install --upgrade pip
pip install -r requirements.txt

# Train
python experiments/scripts/train.py --config configs/base.yaml

# Evaluate
python experiments/scripts/eval.py --config configs/base.yaml


##ğŸ“¬ Contact
For questions or collaboration inquiries, please reach out via GitHub Issues or email.
If you want, I can also **add an ASCII â€œmodule interaction mapâ€** showing exac